{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec training excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this excercise, you will train a Paragraph Vectors / doc2vec model using gensim. You can find information on the gensim doc2vec api here: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "N.B. You should be using Python 3 for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data folder contains a train and test set with small sets of documents from the \"20 newsgroups\" dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're going to do is the following:\n",
    "* Read a dataset with documents\n",
    "* Transform each document into a list of tokens (words)\n",
    "* Train a doc2vec model (DM)\n",
    "* Train a second model (DBOW)\n",
    "* Inspect the outcomes a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import doc2vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic settings\n",
    "HOMEDIR = './'\n",
    "CORPUS_FILE = os.path.join(HOMEDIR, \"data/train_docs.txt\")\n",
    "\n",
    "# file names for the models we'll be creating\n",
    "MODEL_FILE_DM = os.path.join(HOMEDIR, \"models/doc2vec_DM_v20171229.bin\")\n",
    "MODEL_FILE_DBOW = os.path.join(HOMEDIR, \"models/doc2vec_DBOW_v20171229.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the corpus. Each line is a document / paragraph. Optionally preprocess it first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flg_preprocess = False\n",
    "\n",
    "if flg_preprocess:\n",
    "    # quick & simple approach\n",
    "    docs = doc2vec.TaggedLineDocument(CORPUS_FILE)\n",
    "else:\n",
    "    # with pre-processing\n",
    "    with open(CORPUS_FILE, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        docs = [simple_preprocess(line, deacc=False, min_len=1) for line in lines]\n",
    "        docs = [doc2vec.TaggedDocument(doc, tags=[i]) for i, doc in enumerate(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['anarchism', 'is', 'a', 'political', 'philosophy', 'that', 'advocates', 'self', 'governed', 'societies', 'with', 'voluntary', 'institutions', 'these', 'are', 'often', 'described', 'as', 'stateless', 'societies', 'but', 'several', 'authors', 'have', 'defined', 'them', 'more', 'specifically', 'as', 'institutions', 'based', 'on', 'non', 'hierarchical', 'free', 'associations', 'anarchism', 'holds', 'the', 'state', 'to', 'be', 'undesirable', 'unnecessary', 'or', 'harmful', 'while', 'anti', 'statism', 'is', 'central', 'anarchism', 'entails', 'opposing', 'authority', 'or', 'hierarchical', 'organisation', 'in', 'the', 'conduct', 'of', 'human', 'relations', 'including', 'but', 'not', 'limited', 'to', 'the', 'state', 'system'], tags=[0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look at the data\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DM (Distributed Memory) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train DM model\n",
    "model_dm = doc2vec.Doc2Vec(docs, \n",
    "                           vector_size=200, # vector size, should be the same size as pre-trained embedding size when not using dm_concat\n",
    "                           window=10, # window size for word context, on each side\n",
    "                           min_count=1, # minimum nr. of occurrences of a word\n",
    "                           sample=1e-5, # threshold for undersampling high-frequency words\n",
    "                           workers=4, # for multicore processing\n",
    "                           hs=0, # if 1, use hierarchical softmax; if 0, use negative sampling\n",
    "                           dm=1, # if 1 use PV-DM, if 0 use PM-DBOW\n",
    "                           negative=5, # how many words to use for negative sampling\n",
    "                           dbow_words=1, # train word vectors\n",
    "                           dm_concat=1, # concatenate vectors or sum/average them?\n",
    "                           epochs=100 # nr of epochs to train\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it for later use\n",
    "model_dm.save(MODEL_FILE_DM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DBOW (Distributed Bag Of Words) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution_first": true
   },
   "source": [
    "**_Excercise 1: Train a DBOW model_**\n",
    "\n",
    "It's very similar to the previous command. What should you change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# train DBOW model\n",
    "# ...enter your code here...\n",
    "model_dbow = doc2vec.Doc2Vec(docs, \n",
    "                            vector_size=200, # vector size, should be the same size as pre-trained embedding size when not using dm_concat\n",
    "                            window=10, # window size for word context, on each side\n",
    "                            min_count=1, # minimum nr. of occurrences of a word\n",
    "                            sample=1e-5, # threshold for undersampling high-frequency words\n",
    "                            workers=4, # for multicore processing\n",
    "                            hs=0, # if 1, use hierarchical softmax; if 0, use negative sampling\n",
    "                            dm=0, # if 1 use PV-DM, if 0 use PM-DBOW\n",
    "                            negative=5, # how many words to use for negative sampling\n",
    "                            dbow_words=1, # train word vectors\n",
    "                            epochs=100 # nr of epochs to train\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "model_dbow = doc2vec.Doc2Vec(docs, \n",
    "                            size=200, # vector size, should be the same size as pre-trained embedding size when not using dm_concat\n",
    "                            window=10, # window size for word context, on each side\n",
    "                            min_count=1, # minimum nr. of occurrences of a word\n",
    "                            sample=1e-5, # threshold for undersampling high-frequency words\n",
    "                            workers=4, # for multicore processing\n",
    "                            hs=0, # if 1, use hierarchical softmax; if 0, use negative sampling\n",
    "                            dm=0, # if 1 use PV-DM, if 0 use PM-DBOW\n",
    "                            negative=5, # how many words to use for negative sampling\n",
    "                            dbow_words=1, # train word vectors\n",
    "                            iter=100 # nr of epochs to train\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========= END OF EXERCISE ============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also save this one\n",
    "model_dbow.save(MODEL_FILE_DBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question: Look at the model files that are now created in the models directory. Can you explain why there are 2 files for the DM model, but only 1 for the DBOW model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_similar(model, docs, ref_doc_id):\n",
    "    \"\"\"\n",
    "    For a given document, display the most similar ones in the corpus\n",
    "    \"\"\"\n",
    "    def print_doc(doc_id):\n",
    "        doc_txt = ' '.join(docs[doc_id].words)\n",
    "        print(\"[Doc {}]: {}\".format(doc_id, doc_txt))\n",
    "        \n",
    "    print(\"[Original document]\")\n",
    "    print_doc(ref_doc_id)\n",
    "    print(\"\\n[Most similar documents]\")\n",
    "    for doc_id, similarity in model.docvecs.most_similar(ref_doc_id, topn=3):\n",
    "        print(\"-----------------\")\n",
    "        print(\"similarity: {}\".format(similarity))\n",
    "        print_doc(doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original document]\n",
      "[Doc 200]: single scattering albedo is used to define scattering of electromagnetic waves on small particles it depends on properties of the material lrb refractive index rrb the size of the particle or particles and the wavelength of the incoming radiation\n",
      "\n",
      "[Most similar documents]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uSER\\AppData\\Local\\Temp\\ipykernel_34220\\2777213050.py:12: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  for doc_id, similarity in model.docvecs.most_similar(ref_doc_id, topn=3):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "similarity: 0.818175196647644\n",
      "[Doc 165]: the overall albedo of the moon is around but it is strongly directional and non lambertian displaying also a strong opposition effect although such reflectance properties are different from those of any terrestrial terrains they are typical of the regolith surfaces of airless solar system bodies\n",
      "-----------------\n",
      "similarity: 0.811142086982727\n",
      "[Doc 148]: albedo lrb rrb or reflection coefficient derived from latin albedo whiteness lrb or reflected sunlight rrb in turn from albus white is the diffuse reflectivity or reflecting power of a surface\n",
      "-----------------\n",
      "similarity: 0.8111039996147156\n",
      "[Doc 162]: astronomical albedo\n"
     ]
    }
   ],
   "source": [
    "show_most_similar(model_dbow, list(docs), 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = os.path.join(HOMEDIR, \"data/test_docs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data: each line into a list of tokens\n",
    "with open(test_data_file, \"r\") as f:\n",
    "    test_docs = [ x.strip().split() for x in f.readlines() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embeddings for the test documents. Remember: this is an inference step that actually trains a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_docvecs = [model_dm.infer_vector(d, alpha=start_alpha, steps=infer_epoch) for d in test_docs]\n",
    "\n",
    "infer_epoch = 20  # Number of inference epochs\n",
    "\n",
    "test_docvecs = [model_dm.infer_vector(d, alpha=start_alpha, epochs=infer_epoch) for d in test_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.58632842e-03,  3.63510568e-03, -1.21472322e-03, -1.15607830e-03,\n",
       "        7.26221242e-06,  1.88632961e-03, -1.15984026e-03,  2.83681066e-03,\n",
       "        2.13623280e-03, -1.63606170e-03,  6.09889627e-04, -9.48709494e-05,\n",
       "       -2.53189425e-03,  2.51582661e-03, -1.72592211e-03, -4.17035818e-03,\n",
       "        7.85952667e-04, -7.88370598e-05,  3.83347913e-04, -1.40118389e-03,\n",
       "        2.93286459e-04,  3.21178860e-03,  1.74682192e-03, -4.14444925e-03,\n",
       "       -1.47055055e-03, -2.33964832e-03,  1.89658953e-03, -1.56889553e-03,\n",
       "        1.53461634e-03, -1.19663282e-05, -5.22329099e-03,  2.03342712e-03,\n",
       "        2.00471375e-03, -8.12001061e-03,  1.19660384e-04, -3.38673429e-03,\n",
       "        1.12576061e-03, -5.53413620e-03, -4.18214547e-03, -3.31581896e-03,\n",
       "        1.56260678e-03, -4.57505463e-03,  3.84328421e-03, -1.67247851e-03,\n",
       "        1.23591931e-03, -2.89132074e-03, -4.92465962e-03,  1.56075251e-03,\n",
       "        3.86150088e-03, -1.12858892e-03,  3.01103736e-03,  3.27742528e-05,\n",
       "        1.61835662e-04, -2.16657689e-04,  3.71199823e-03,  5.03032189e-03,\n",
       "       -2.76843924e-03,  1.14906253e-03, -2.59988941e-03, -1.66411488e-03,\n",
       "        5.11548761e-03, -2.39296793e-03, -3.95618053e-03, -5.55685814e-03,\n",
       "       -5.02352370e-03, -2.81975023e-03,  9.23786953e-04, -2.18541827e-03,\n",
       "       -7.52274122e-04, -5.48135070e-03,  3.04651563e-03, -1.81282894e-03,\n",
       "        6.76631869e-04,  1.49678803e-04,  2.32552853e-03, -8.95252044e-04,\n",
       "        9.45280990e-05,  1.71929703e-03, -2.05398700e-03, -1.71893020e-03,\n",
       "        2.23078905e-03, -1.84505386e-03,  5.21281559e-04,  2.57136562e-04,\n",
       "        1.63064490e-03,  2.49665929e-04,  4.85717924e-03,  1.67348608e-03,\n",
       "        2.56082905e-03,  9.57008859e-04, -1.02496298e-03, -1.03639078e-03,\n",
       "        1.53123785e-03, -9.98645672e-04, -3.42307868e-03,  4.02478734e-03,\n",
       "       -1.31693122e-03,  2.17395253e-03, -3.12495953e-03,  6.14507007e-04,\n",
       "        2.11210409e-03, -7.58875581e-03, -3.36463097e-03, -3.76612949e-03,\n",
       "        8.81141867e-04,  6.01647829e-04, -4.34489129e-03, -4.98405844e-03,\n",
       "        2.78118096e-04, -4.59321449e-03,  3.01631959e-03, -2.54311948e-03,\n",
       "       -2.72717723e-03, -4.95924009e-03,  3.23187280e-03, -3.44741886e-04,\n",
       "       -5.59586973e-04,  4.00935579e-03, -5.59571199e-03, -4.73112334e-03,\n",
       "        6.58653502e-04,  1.05283980e-03, -1.67549634e-03, -4.06478764e-03,\n",
       "        4.55348892e-03, -1.69134396e-03, -1.16003549e-03, -2.51196185e-03,\n",
       "       -3.08953016e-03,  9.95131486e-05, -6.59809390e-04, -4.38965624e-03,\n",
       "        1.31301919e-03, -1.20703678e-03,  3.62744578e-03,  3.56110948e-04,\n",
       "       -4.08879947e-03,  7.38361198e-03, -3.78481601e-03, -2.33267900e-03,\n",
       "       -2.40616989e-03, -1.97128463e-03,  1.65837177e-03,  1.19164388e-03,\n",
       "       -2.15088646e-03,  2.02705758e-03, -2.16202578e-03,  4.70132055e-03,\n",
       "        3.29381903e-03, -2.77377595e-03, -4.90364910e-04,  5.40596782e-04,\n",
       "       -5.85301500e-03,  4.33558738e-03,  2.09719874e-03, -6.68898318e-03,\n",
       "        4.71002469e-03,  6.49912423e-03,  6.17159647e-04,  4.72309487e-03,\n",
       "       -1.63236586e-03, -1.00177433e-03, -2.00206880e-03,  4.67394339e-03,\n",
       "        5.28690638e-04, -6.34086598e-03,  4.65534860e-04,  2.20853323e-03,\n",
       "       -1.46409683e-03, -1.78817881e-03, -3.68296844e-03,  1.04393694e-03,\n",
       "       -9.21211787e-04, -2.82574911e-03,  5.72606041e-05, -2.07991037e-03,\n",
       "        1.47490005e-03, -8.46373849e-04, -6.12200517e-03,  1.75381021e-03,\n",
       "        1.01003610e-03, -1.89272198e-03, -4.27281857e-03,  6.54342235e-04,\n",
       "        1.98709522e-03, -1.60033605e-03, -3.05857137e-03, -4.36346699e-03,\n",
       "       -2.56861048e-03, -6.33937400e-03, -5.56247658e-04,  5.50953555e-05,\n",
       "        2.46447884e-03, -3.88809387e-03, -8.41367175e-04, -3.61045054e-03,\n",
       "       -1.57906639e-03,  2.68903282e-03,  4.57157940e-03, -5.62511804e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see what one document embedding looks like\n",
    "test_docvecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The excercise continues in the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
