{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec training excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this excercise, you will train a Paragraph Vectors / doc2vec model using gensim. You can find information on the gensim doc2vec api here: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "N.B. You should be using Python 3 for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data folder contains a train and test set with small sets of documents from the \"20 newsgroups\" dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're going to do is the following:\n",
    "* Read a dataset with documents\n",
    "* Transform each document into a list of tokens (words)\n",
    "* Train a doc2vec model (DM)\n",
    "* Train a second model (DBOW)\n",
    "* Inspect the outcomes a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import doc2vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic settings\n",
    "HOMEDIR = './'\n",
    "CORPUS_FILE = os.path.join(HOMEDIR, \"data/corpus_train.txt\")\n",
    "\n",
    "# file names for the models we'll be creating\n",
    "MODEL_FILE_DM = os.path.join(HOMEDIR, \"models/doc2vec_DM_v20171229.bin\")\n",
    "MODEL_FILE_DBOW = os.path.join(HOMEDIR, \"models/doc2vec_DBOW_v20171229.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the corpus. Each line is a document / paragraph. Optionally preprocess it first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flg_preprocess = False\n",
    "\n",
    "if flg_preprocess:\n",
    "    # quick & simple approach\n",
    "    docs = doc2vec.TaggedLineDocument(CORPUS_FILE)\n",
    "else:\n",
    "    # with pre-processing\n",
    "    with open(CORPUS_FILE, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        docs = [simple_preprocess(line, deacc=False, min_len=1) for line in lines]\n",
    "        docs = [doc2vec.TaggedDocument(doc, tags=[i]) for i, doc in enumerate(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['どうかを見極めましょう', 'なるべく新しい情報を集める', '情報は日々更新されています', '特にイ'], tags=[0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look at the data\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DM (Distributed Memory) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train DM model\n",
    "model_dm = doc2vec.Doc2Vec(docs, \n",
    "                           vector_size=200, # vector size, should be the same size as pre-trained embedding size when not using dm_concat\n",
    "                           window=10, # window size for word context, on each side\n",
    "                           min_count=1, # minimum nr. of occurrences of a word\n",
    "                           sample=1e-5, # threshold for undersampling high-frequency words\n",
    "                           workers=4, # for multicore processing\n",
    "                           hs=0, # if 1, use hierarchical softmax; if 0, use negative sampling\n",
    "                           dm=1, # if 1 use PV-DM, if 0 use PM-DBOW\n",
    "                           negative=5, # how many words to use for negative sampling\n",
    "                           dbow_words=1, # train word vectors\n",
    "                           dm_concat=1, # concatenate vectors or sum/average them?\n",
    "                           epochs=100 # nr of epochs to train\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it for later use\n",
    "model_dm.save(MODEL_FILE_DM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DBOW (Distributed Bag Of Words) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution_first": true
   },
   "source": [
    "**_Excercise 1: Train a DBOW model_**\n",
    "\n",
    "It's very similar to the previous command. What should you change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# train DBOW model\n",
    "# ...enter your code here...\n",
    "model_dbow = doc2vec.Doc2Vec(docs, \n",
    "                            vector_size=200, # vector size, should be the same size as pre-trained embedding size when not using dm_concat\n",
    "                            window=10, # window size for word context, on each side\n",
    "                            min_count=1, # minimum nr. of occurrences of a word\n",
    "                            sample=1e-5, # threshold for undersampling high-frequency words\n",
    "                            workers=4, # for multicore processing\n",
    "                            hs=0, # if 1, use hierarchical softmax; if 0, use negative sampling\n",
    "                            dm=0, # if 1 use PV-DM, if 0 use PM-DBOW\n",
    "                            negative=5, # how many words to use for negative sampling\n",
    "                            dbow_words=1, # train word vectors\n",
    "                            epochs=100 # nr of epochs to train\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# solution *This is code of an older version. do not run*\n",
    "model_dbow = doc2vec.Doc2Vec(docs, \n",
    "                            size=200, # vector size, should be the same size as pre-trained embedding size when not using dm_concat\n",
    "                            window=10, # window size for word context, on each side\n",
    "                            min_count=1, # minimum nr. of occurrences of a word\n",
    "                            sample=1e-5, # threshold for undersampling high-frequency words\n",
    "                            workers=4, # for multicore processing\n",
    "                            hs=0, # if 1, use hierarchical softmax; if 0, use negative sampling\n",
    "                            dm=0, # if 1 use PV-DM, if 0 use PM-DBOW\n",
    "                            negative=5, # how many words to use for negative sampling\n",
    "                            dbow_words=1, # train word vectors\n",
    "                            iter=100 # nr of epochs to train\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========= END OF EXERCISE ============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also save this one\n",
    "model_dbow.save(MODEL_FILE_DBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question: Look at the model files that are now created in the models directory. Can you explain why there are 2 files for the DM model, but only 1 for the DBOW model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_similar(model, docs, ref_doc_id):\n",
    "    \"\"\"\n",
    "    For a given document, display the most similar ones in the corpus\n",
    "    \"\"\"\n",
    "    def print_doc(doc_id):\n",
    "        doc_txt = ' '.join(docs[doc_id].words)\n",
    "        print(\"[Doc {}]: {}\".format(doc_id, doc_txt))\n",
    "        \n",
    "    print(\"[Original document]\")\n",
    "    print_doc(ref_doc_id)\n",
    "    print(\"\\n[Most similar documents]\")\n",
    "    for doc_id, similarity in model.docvecs.most_similar(ref_doc_id, topn=3):\n",
    "        print(\"-----------------\")\n",
    "        print(\"similarity: {}\".format(similarity))\n",
    "        print_doc(doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original document]\n",
      "[Doc 200]: 体的な対策案を詳しく解説します html を定期的に変更する サイトの html を定期的に変更す\n",
      "\n",
      "[Most similar documents]\n",
      "-----------------\n",
      "similarity: 0.976435661315918\n",
      "[Doc 7319]: サイトの html が書き換えられても web スクレイピングできる web スクレイピングで取得\n",
      "-----------------\n",
      "similarity: 0.9718865156173706\n",
      "[Doc 15348]: 穴が空いてしまった 残念ながら サイトの html 構造の\n",
      "-----------------\n",
      "similarity: 0.9671600461006165\n",
      "[Doc 7341]: web サイトの html が書き換えられ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uSER\\AppData\\Local\\Temp\\ipykernel_45768\\2777213050.py:12: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  for doc_id, similarity in model.docvecs.most_similar(ref_doc_id, topn=3):\n"
     ]
    }
   ],
   "source": [
    "show_most_similar(model_dbow, list(docs), 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = os.path.join(HOMEDIR, \"data/corpus_total.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data: each line into a list of tokens\n",
    "with open(test_data_file, \"r\") as f:\n",
    "    test_docs = [ x.strip().split() for x in f.readlines() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embeddings for the test documents. Remember: this is an inference step that actually trains a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_docvecs = [model_dm.infer_vector(d, alpha=start_alpha, steps=infer_epoch) for d in test_docs]\n",
    "\n",
    "infer_epoch = 20  # Number of inference epochs\n",
    "\n",
    "test_docvecs = [model_dm.infer_vector(d, alpha=start_alpha, epochs=infer_epoch) for d in test_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.1745462e-03, -1.3884333e-03, -2.1389707e-03,  2.7547378e-04,\n",
       "       -1.6884271e-03,  9.8828354e-04,  2.6889006e-03,  1.8476255e-03,\n",
       "        9.7175839e-04, -1.4412728e-03,  1.1199429e-03,  5.7260576e-04,\n",
       "        2.0723555e-03,  1.4076020e-03,  1.6640181e-03,  3.1390295e-03,\n",
       "       -1.1036033e-03, -1.6768518e-03, -2.2916360e-04, -1.3597443e-04,\n",
       "        1.2440752e-04, -1.3818053e-03,  1.0196362e-03,  1.6079628e-03,\n",
       "        3.5036361e-04,  8.6599769e-04,  2.1145046e-03, -1.8057314e-03,\n",
       "       -1.4154599e-03,  2.8959394e-04, -3.2196488e-04,  3.0237322e-03,\n",
       "        1.6618476e-03,  9.0015819e-05,  1.7056705e-03, -1.3774577e-04,\n",
       "        1.0094722e-03, -1.5144229e-04,  2.0610006e-03, -3.4465315e-03,\n",
       "        1.3685275e-03, -2.2041015e-03,  1.0888297e-03, -7.2553717e-05,\n",
       "       -1.0431758e-03, -2.8383040e-03,  8.6026359e-04, -3.1328239e-03,\n",
       "        6.2263738e-05,  1.2314832e-03, -4.3906824e-04, -2.2127018e-03,\n",
       "        2.0221604e-03,  1.3907196e-03,  1.8366361e-03, -4.3923350e-04,\n",
       "       -4.8335514e-05, -1.6069515e-03, -5.3927518e-04, -1.5378627e-03,\n",
       "       -3.1347240e-03,  2.9386124e-03,  2.7966115e-03, -6.7924999e-04,\n",
       "        3.6804267e-04,  2.4243966e-03, -1.2150637e-03,  3.2884395e-03,\n",
       "       -5.0249870e-04, -1.9125844e-03,  3.7249480e-04,  2.9721227e-04,\n",
       "        1.5317439e-03, -2.4825493e-03,  3.4674545e-04,  1.0726803e-03,\n",
       "       -2.1986666e-04,  2.4930434e-03,  8.9033181e-04, -2.3531490e-03,\n",
       "       -1.6957404e-03,  6.3793070e-04, -6.2556943e-04, -4.4779730e-04,\n",
       "       -8.2551345e-04,  1.3624755e-04, -7.2458461e-05, -1.3135289e-03,\n",
       "       -2.9169911e-05, -1.2219789e-03, -2.8792375e-03,  1.5854050e-03,\n",
       "        5.3559593e-04,  1.2556369e-03,  1.3066527e-03,  4.9422683e-06,\n",
       "        2.1133975e-03,  1.4219935e-03,  6.1187777e-05,  2.2588097e-03,\n",
       "       -7.6093688e-04,  1.5211591e-03, -2.4311000e-03, -1.0677640e-03,\n",
       "        1.3336124e-03,  2.1576656e-03,  1.4802595e-03, -1.0135075e-03,\n",
       "        8.8580500e-04,  2.3731149e-03,  1.6734193e-03, -8.4165961e-04,\n",
       "        3.9965508e-04, -6.7841471e-04, -3.1043824e-03, -6.4612314e-04,\n",
       "        5.0791173e-04, -1.6828618e-04,  1.4906556e-04,  1.1947833e-03,\n",
       "        3.8198696e-04,  1.9975204e-04, -2.4821686e-03, -5.4921507e-05,\n",
       "       -1.3270692e-03, -6.5189571e-04, -1.7742938e-03, -4.1653364e-04,\n",
       "       -6.1153434e-04,  6.8496022e-04, -2.0361331e-04, -1.6520683e-03,\n",
       "       -1.8634916e-03,  3.1357422e-03, -9.5795869e-04, -5.4961524e-04,\n",
       "        4.5128012e-04,  1.8969490e-03,  1.4817821e-03,  3.0637221e-04,\n",
       "       -3.5292646e-04,  2.0056435e-04,  2.2407968e-03,  3.0850811e-04,\n",
       "       -3.6180380e-03,  1.3524919e-03,  2.2973989e-03, -1.6155399e-03,\n",
       "        9.5279596e-04,  1.9267426e-03,  3.9883735e-04,  3.5605812e-04,\n",
       "       -1.3906231e-04,  1.2116130e-03, -2.5143567e-03, -1.7469954e-03,\n",
       "        2.0206957e-03,  4.1005950e-04, -3.3364536e-03,  1.8393692e-03,\n",
       "        2.7012322e-03, -1.1704335e-03, -1.1212071e-03,  2.1009794e-03,\n",
       "       -1.4847933e-03, -9.2093885e-04, -1.7333154e-03,  2.5962768e-03,\n",
       "        1.0966787e-03, -1.5388117e-03, -1.4209682e-03, -6.3400058e-04,\n",
       "       -8.5797132e-04, -2.8936369e-03, -1.5895333e-03,  1.0294196e-03,\n",
       "       -1.3876983e-03, -2.2572551e-04,  9.0081239e-04, -2.9099532e-03,\n",
       "       -1.9189687e-03,  1.3740973e-03, -1.6422390e-03,  6.7277302e-05,\n",
       "        2.9758469e-03, -6.3335814e-04,  9.5959590e-04,  2.1415215e-05,\n",
       "       -2.0840310e-03, -4.9717765e-04, -2.1881843e-03, -1.3785674e-03,\n",
       "        8.4288965e-04, -7.4175437e-04,  6.5144768e-04, -1.7531742e-03,\n",
       "        2.3250764e-03, -3.4784134e-03, -9.4739815e-05,  1.7191347e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see what one document embedding looks like\n",
    "test_docvecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The excercise continues in the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
